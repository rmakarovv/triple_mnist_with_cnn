{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "    author: Roman Makarov\n",
        "    e-mail: mcronomus@gmail.com"
      ],
      "metadata": {
        "id": "MQ2QlCf3zAyx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGMrDL5JLPU9"
      },
      "source": [
        "#CNN for Triple MNIST classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Data preparation and creation of data loaders"
      ],
      "metadata": {
        "id": "rY6Z22G42RoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1DtWT_0JuLzvAoysUcH0j5mbcVFL7Y45T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSs61Tz-ckOw",
        "outputId": "88e6ae96-cf46-4992-b237-ab9ccec97963"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DtWT_0JuLzvAoysUcH0j5mbcVFL7Y45T\n",
            "To: /content/triple_mnist.zip\n",
            "100% 85.7M/85.7M [00:00<00:00, 115MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip triple_mnist.zip"
      ],
      "metadata": {
        "id": "FlKZD3JPaHM6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Cszd-0-gIETQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "img_path = 'triple_mnist'\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, img_path, transform, to_perform=False, transform_2=-1):\n",
        "    self.path = img_path\n",
        "    self.folder = [x for x in os.listdir(img_path)]\n",
        "\n",
        "    self.images = []\n",
        "    for x in self.folder:\n",
        "        for y in os.listdir(img_path + '/' + x):\n",
        "            self.images.append((img_path + '/' + x + '/' + y, 0))\n",
        "\n",
        "            if type(transform_2) is not int:\n",
        "                self.images.append((img_path + '/' + x + '/' + y, 1))\n",
        "\n",
        "    self.transform = transform\n",
        "    self.transform_2 = transform_2\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_loc = self.images[idx][0]\n",
        "    type_ = self.images[idx][1]\n",
        "\n",
        "    image = Image.open(img_loc).convert('RGB')\n",
        "\n",
        "    if type_ == 0 or type(self.transform_2) is int:\n",
        "        single_img = self.transform(image)\n",
        "    else:\n",
        "        single_img = self.transform_2(image)\n",
        "\n",
        "    general_label = self.images[idx][0].split('/')[-1].split('.')[0].split('_')[-1]\n",
        "\n",
        "    label1 = int(general_label[0])\n",
        "    label2 = int(general_label[1])\n",
        "    label3 = int(general_label[2])\n",
        "\n",
        "    sample = {'image':single_img, 'labels': {0:label1, 1:label2, 2:label3}}\n",
        "\n",
        "    return sample"
      ],
      "metadata": {
        "id": "XPdcHgEpcRnI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(inp):\n",
        "    inp = inp.numpy()[0]\n",
        "    mean = 0.1307\n",
        "    std = 0.3081\n",
        "    inp = inp * std + mean\n",
        "    plt.imshow(inp, cmap='gray')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5fiKNeDjA3h6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_train = 128\n",
        "batch_size_test = 128\n",
        "batch_size_val = 128\n",
        "\n",
        "data_transforms = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
        "                                      transforms.ToTensor()])\n",
        "\n",
        "data_transforms_train = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
        "                                            transforms.RandomRotation(15),\n",
        "                                            transforms.Resize((84, 84)),\n",
        "                                            transforms.ToTensor()])\n",
        "train_dataset = ImageDataset(img_path + '/train', data_transforms, True, data_transforms_train)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True,\n",
        "                                num_workers=2)\n",
        "\n",
        "test_dataset = ImageDataset(img_path + '/test', data_transforms, False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=True,\n",
        "                               num_workers=2)\n",
        "\n",
        "val_dataset = ImageDataset(img_path + '/val', data_transforms, False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size_val, shuffle=True,\n",
        "                               num_workers=2)"
      ],
      "metadata": {
        "id": "gx7411QXc2uA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Creating CNN model and testing it on dataset"
      ],
      "metadata": {
        "id": "V4C_9fV82a7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "import numpy as np\n",
        "\n",
        "def train(model, device, epochs, train_loader, loss_func):\n",
        "    n_total_steps = len(train_loader)\n",
        "    start_time = time()\n",
        "\n",
        "    correct = 0\n",
        "    all = 0\n",
        "\n",
        "    for i, pictures in enumerate(train_loader):\n",
        "        images = pictures['image'].to(device)\n",
        "\n",
        "        outputs = model(images).to(device)\n",
        "\n",
        "        loss = 0\n",
        "        correct_cur = np.zeros((outputs.shape[0], 3))\n",
        "\n",
        "        for ii in range(3):\n",
        "            outputs_i = outputs[:, ii * 10 : (ii + 1) * 10]\n",
        "            pred = outputs_i.argmax(dim=1, keepdim=True).to(device)\n",
        "\n",
        "            correct_pred = pred.eq(pictures['labels'][ii].view_as(pred).to(device)).cpu().detach().numpy().astype(int)\n",
        "            for j in range(outputs.shape[1]):\n",
        "                correct_cur[j][ii] += correct_pred[j]\n",
        "                if ii == 2 and correct_cur[j].sum() == 3:\n",
        "                    correct += 1\n",
        "\n",
        "            loss += loss_func(outputs[:, ii * 10 : (ii + 1) * 10], pictures['labels'][ii].to(device)).to(device)\n",
        "\n",
        "        all += outputs.shape[1]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(f'\\rTrain Epoch [{epoch}],',\n",
        "                    f'Step [{i + 1}/{n_total_steps}],',\n",
        "                    f'Loss: {loss:.2f},',\n",
        "                    f'Accuracy: {100. * correct / all:.2f}%',\n",
        "                    f'elapsed time: {time() - start_time:.2f}s',\n",
        "                    end='')\n",
        "    print()\n",
        "\n",
        "def test(model, device, test_loader, loss_func, type_of_test='Test'):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    all = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, pictures in enumerate(test_loader):\n",
        "            images = pictures['image'].to(device)\n",
        "            pictures = pictures\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            correct_cur = np.zeros((outputs.shape[0], 3))\n",
        "\n",
        "            for ii in range(3):\n",
        "                outputs_i = outputs[:, ii * 10 : (ii + 1) * 10]\n",
        "                pred = outputs_i.argmax(dim=1, keepdim=True)\n",
        "\n",
        "                correct_pred = pred.eq(pictures['labels'][ii].view_as(pred).to(device)).cpu().detach().numpy().astype(int)\n",
        "                for j in range(outputs.shape[1]):\n",
        "                    correct_cur[j][ii] += correct_pred[j]\n",
        "                    if ii == 2 and correct_cur[j].sum() == 3:\n",
        "                        correct += 1\n",
        "\n",
        "                test_loss += loss_func(outputs[:, ii * 10 : (ii + 1) * 10], pictures['labels'][ii].to(device))\n",
        "\n",
        "            all += outputs.shape[1]\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(f'{type_of_test} set: Average loss: {test_loss:.2f}, Accuracy: {100. * correct / all:.2f}%')\n",
        "\n",
        "    return 100. * correct / all"
      ],
      "metadata": {
        "id": "LNKZI5Pr1zrl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Model with 3 convolutional layers"
      ],
      "metadata": {
        "id": "gi5VUgkcDNLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, n_digits=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.general = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=24, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.Conv2d(in_channels=24, out_channels=24, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.BatchNorm2d(24),\n",
        "            nn.Conv2d(in_channels=24, out_channels=12, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.BatchNorm2d(12),\n",
        "        )\n",
        "\n",
        "        self.label = nn.Sequential(\n",
        "            nn.Linear(in_features = 10 * 10 * 12, out_features = 64),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features = 64, out_features = n_digits * 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.general(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.label(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-7VCji0ijrhU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_train = 128\n",
        "batch_size_test = 128\n",
        "batch_size_val = 128\n",
        "\n",
        "data_transforms = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
        "                                      transforms.ToTensor()])\n",
        "\n",
        "data_transforms_train = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
        "                                            transforms.RandomRotation(15),\n",
        "                                            transforms.Resize((84, 84)),\n",
        "                                            transforms.ToTensor()])\n",
        "\n",
        "train_dataset = ImageDataset(img_path + '/train', data_transforms, True, data_transforms_train)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True,\n",
        "                                num_workers=2)\n",
        "\n",
        "test_dataset = ImageDataset(img_path + '/test', data_transforms, False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=True,\n",
        "                               num_workers=2)\n",
        "\n",
        "val_dataset = ImageDataset(img_path + '/val', data_transforms, False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size_val, shuffle=True,\n",
        "                               num_workers=2)\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "lr = 0.005\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(10).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "epoch_nums = []\n",
        "validation_loss = []\n",
        "current_count = 0\n",
        "\n",
        "epochs = 10\n",
        "print('Training on:', device, '\\n')\n",
        "for epoch in range(1, epochs + 1):\n",
        "        train(model=model, device=device, train_loader=train_loader, epochs=epoch, loss_func=loss_func)\n",
        "\n",
        "        test_accuracy = test(model=model, device=device, test_loader=test_loader, loss_func=loss_func)\n",
        "\n",
        "        if epoch > 1:\n",
        "            if test_accuracy - validation_loss[-1] < 0.5:\n",
        "                current_count += 1\n",
        "            else:\n",
        "                current_count = 0\n",
        "\n",
        "        epoch_nums.append(epoch)\n",
        "        validation_loss.append(test_accuracy)\n",
        "\n",
        "        if current_count > 1:\n",
        "            print(f'Early stopping on epoch: {epoch}')\n",
        "            break\n",
        "\n",
        "print('\\nProceeding to validation...')\n",
        "val_acc = test(model=model, device=device, test_loader=val_loader, loss_func=loss_func, type_of_test='Validation')"
      ],
      "metadata": {
        "id": "biFayTjtVALv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175654fb-b685-4da8-d70f-52f6716a0f71"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on: cuda \n",
            "\n",
            "Train Epoch [1], Step [1000/1000], Loss: 0.46, Accuracy: 76.77% elapsed time: 65.18s\n",
            "Test set: Average loss: 0.01, Accuracy: 66.77%\n",
            "Train Epoch [2], Step [1000/1000], Loss: 0.15, Accuracy: 88.26% elapsed time: 51.68s\n",
            "Test set: Average loss: 0.00, Accuracy: 91.89%\n",
            "Train Epoch [3], Step [1000/1000], Loss: 0.27, Accuracy: 92.80% elapsed time: 51.48s\n",
            "Test set: Average loss: 0.00, Accuracy: 93.27%\n",
            "Train Epoch [4], Step [1000/1000], Loss: 0.17, Accuracy: 93.48% elapsed time: 51.68s\n",
            "Test set: Average loss: 0.00, Accuracy: 93.08%\n",
            "Train Epoch [5], Step [1000/1000], Loss: 0.12, Accuracy: 93.96% elapsed time: 52.13s\n",
            "Test set: Average loss: 0.00, Accuracy: 94.54%\n",
            "Train Epoch [6], Step [1000/1000], Loss: 0.20, Accuracy: 94.16% elapsed time: 51.79s\n",
            "Test set: Average loss: 0.00, Accuracy: 94.25%\n",
            "Train Epoch [7], Step [1000/1000], Loss: 0.15, Accuracy: 94.26% elapsed time: 52.10s\n",
            "Test set: Average loss: 0.00, Accuracy: 93.99%\n",
            "Early stopping on epoch: 7\n",
            "\n",
            "Proceeding to validation...\n",
            "Validation set: Average loss: 0.00, Accuracy: 94.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Demonstrating an example"
      ],
      "metadata": {
        "id": "8MqGhDv1CPZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "res = next(iter(train_loader))\n",
        "images = res['image'].to(device)\n",
        "labels = res['labels']\n",
        "\n",
        "def get_label(labels, idx):\n",
        "    return f\"{labels[0][idx].item()}{labels[1][idx].item()}{labels[2][idx].item()}\"\n",
        "\n",
        "print('Label=', get_label(labels, 0))\n",
        "\n",
        "predict = model(images)[0]\n",
        "p_v = []\n",
        "for i in range(3):\n",
        "    predict_i = predict[i * 10 : (i + 1) * 10]\n",
        "    pred = predict_i.argmax(dim=0, keepdim=True)\n",
        "    p_v.append(pred.item())\n",
        "\n",
        "print(f'Predicted label {p_v[0]}{p_v[1]}{p_v[2]}')\n",
        "\n",
        "imshow(images[0].cpu())"
      ],
      "metadata": {
        "id": "xKbflaaw8Mgo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "56394da8-b3ea-45ab-c26e-0c1d0b95d77c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label= 581\n",
            "Predicted label 581\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Measuring time taken to predict 50 samples"
      ],
      "metadata": {
        "id": "TrjnjXBzmmwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "\n",
        "def measure(model, device, test_loader, task_n=1):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    all = 0\n",
        "\n",
        "    start = time()\n",
        "    with torch.no_grad():\n",
        "        for i, pictures in enumerate(test_loader):\n",
        "            images = pictures['image'].to(device)\n",
        "            pictures = pictures\n",
        "\n",
        "            outputs = model(images)\n",
        "            break\n",
        "\n",
        "    end = time()\n",
        "    print(f'Time to predict 50 samples for CNN model: {end - start:.2f}s')"
      ],
      "metadata": {
        "id": "vquYODBUejcj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_train = 50\n",
        "\n",
        "data_transforms = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n",
        "                                      transforms.ToTensor()])\n",
        "\n",
        "measure_dataset = ImageDataset(img_path + '/train', data_transforms, False)\n",
        "\n",
        "measure_loader = DataLoader(measure_dataset, batch_size=batch_size_train, shuffle=True,\n",
        "                                num_workers=2)\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "lr = 0.005\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(10).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "epoch_nums = []\n",
        "validation_loss = []\n",
        "current_count = 0\n",
        "\n",
        "measure(model=model, device=device, test_loader=measure_loader, task_n=2)"
      ],
      "metadata": {
        "id": "Jq47K44uggFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c940fb6-a42c-45eb-96dc-f092185645cb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to predict 50 samples for CNN model: 0.24s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}